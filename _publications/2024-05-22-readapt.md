---
title: 'RE-Adapt: Reverse Engineered Adaptation of Large Language Models'
collection: publications
permalink: /publication/readapt
excerpt: 'We introduce RE-Adapt, an approach to fine-tuning large language models on new domains without degrading any pre-existing instruction-tuning. We reverse engineer an adapter which isolates what an instruction-tuned model has learned beyond its corresponding pretrained base model. Importantly, this requires no additional data or training. We can then fine-tune the base model on a new domain and readapt it to instruction following with the reverse engineered adapter. REAdapt and our low-rank variant LoRE-Adapt both outperform other methods of fine-tuning, across multiple popular LLMs and datasets, even when the models are used in conjunction with retrieval-augmented generation.'
date: 2024-05-22
venue: ArXiv
citation: 'William Fleshman and Benjamin Van Durme, RE-Adapt: Reverse Engineered Adaptation of Large Language Models, 2024.'
paperurl: 'https://fleshman.dev/files/readapt.pdf'
---
